dataset_directory: dataset/
weights_directory: weights/
logging_directory: logs/

environment:  # Environment settings
  name: LowerT1KickToTarget-v0  # Options: LowerT1PenaltyKick-v0, LowerT1GoaliePenaltyKick-v0, LowerT1ObstaclePenaltyKick-v0, LowerT1KickToTarget-v0
  headless: False
  max_episode_steps: 1000

model:
  states: 52
  actions: 12

teleop:  # Teleoperation data collection settings
  enabled: False
  position_sensitivity: 0.01
  rotation_sensitivity: 1.5
  file_prefix: teleop_data

visualize:  # Visualization settings
  enabled: False
  weight_file: behavior_cloning-1764496680.031678.pt

behavior_cloning:  # Supervised Imitation Learning settings
  enabled: False
  batch_size: 64
  epochs: 200
  learning_rate: 0.0005
  data_files:
    - teleop_data-1764112264.3227012.npz
    - part_1.npz
    - part_2.npz
    - part_3.npz
    - part_4.npz
  file_prefix: behavior_cloning
  weight_file: behavior_cloning-1764498522.4837887.pt

ppo:  # Proximal Policy Optimization settings
  enabled: True
  task: WalkToBall  # WalkToBall, PenaltyKick, GoaliePenaltyKick, ObstaclePenaltyKick, KickToTarget
  epochs: 1000         # Number of times to update the policy using the collected data
  batch_size: 64     # Size of mini-batch for gradient descent
  clip_epsilon: 0.2      # Clipping parameter (epsilon)
  gamma: 0.99            # Discount factor
  lmbda: 0.95       # GAE factor
  entropy_eps: 0.0001      # Entropy coeff
  value_loss_coeff: 0.5    # Value Loss Coeff
  max_grad_norm: 1.0       # Max gradient norm for clipping
  learning_rate: 0.0005               # Learning rate
  total_frames: 10000
  frames_per_batch: 100
  weight_file: behavior_cloning-1764498522.4837887.pt
  file_prefix: PPO
