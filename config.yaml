dataset_directory: dataset/
weights_directory: weights/
logging_directory: logs/

environment:  # Environment settings
  name: LowerT1KickToTarget-v0  # Options: LowerT1KickToTarget-v0, LowerT1GoaliePenaltyKick-v0, LowerT1ObstaclePenaltyKick-v0, LowerT1PenaltyKick-v0
  headless: False
  max_episode_steps: 500

model:
  states: 52
  actions: 12

teleop:  # Teleoperation data collection settings
  enabled: False
  position_sensitivity: 0.1
  rotation_sensitivity: 1.5
  file_prefix: teleop_data

visualize:  # Visualization settings
  enabled: True
  weight_file: behavior_cloning-1764142244.46165.pt

behavior_cloning:  # Supervised Imitation Learning settings
  enabled: False
  batch_size: 64
  epochs: 150
  learning_rate: 0.0004
  data_file: teleop_data-1764112264.3227012.npz
  file_prefix: behavior_cloning

ppo:  # Proximal Policy Optimization settings
  enabled: False
  epochs: 50         # Number of times to update the policy using the collected data
  batch_size: 64     # Size of mini-batch for gradient descent
  clip_epsilon: 0.2      # Clipping parameter (epsilon)
  gamma: 0.99            # Discount factor
  gae_lambda: 0.95       # GAE factor
  entropy_beta: 0.01      # Entropy coeff
  value_loss_coeff: 0.5    # Value Loss Coeff
  learning_rate: 0.0003               # Learning rate
  weight_file: behavior_cloning-1764142244.46165.pt
  file_prefix: ppo
