dataset_directory: dataset/
weights_directory: weights/
logging_directory: logs/

environment:  # Environment settings
  name: LowerT1KickToTarget-v0  # Options: LowerT1PenaltyKick-v0, LowerT1GoaliePenaltyKick-v0, LowerT1ObstaclePenaltyKick-v0, LowerT1KickToTarget-v0
  headless: False
  max_episode_steps: 1000

model:
  states: 52
  actions: 12

teleop:  # Teleoperation data collection settings
  enabled: False
  position_sensitivity: 0.01
  rotation_sensitivity: 1.5
  file_prefix: teleop_data2

visualize:  # Visualization settings
  enabled: True
  weight_file: PPO-1768228695.7902398.pt

behavior_cloning:  # Supervised Imitation Learning settings
  enabled: False
  batch_size: 32
  epochs: 50
  learning_rate: 0.001
  data_files:
    - teleop_data-1764112264.3227012.npz
    - part_1.npz
    - part_2.npz
    - part_3.npz
    - part_4.npz
  file_prefix: BC
  weight_file: BC-1765086359.1442845.pt

ppo:  # Proximal Policy Optimization settings
  enabled: False
  task: WalkToBall   # WalkToBall, PenaltyKick, GoaliePenaltyKick, ObstaclePenaltyKick, KickToTarget
  num_envs: 16        # Number of parallel environments
  epochs: 10         # Number of times to update the policy using the collected data
  batch_size: 64     # Size of mini-batch for gradient descent
  clip_epsilon: 0.25     # Clipping parameter (epsilon)
  gamma: 0.99            # Discount factor
  lmbda: 0.95        # GAE factor
  entropy_eps: 0.001      # Entropy coeff
  value_loss_coeff: 0.5    # Value Loss Coeff
  max_grad_norm: 1.0       # Max gradient norm for clipping
  learning_rate: 0.001               # Learning rate
  total_frames: 10000000
  frames_per_batch: 10000
  weight_file: BC-1765086359.1442845.pt
  file_prefix: PPO
